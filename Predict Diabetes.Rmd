---
title: <center>Predict Diabetes </center> 
author:  <center>Huynh | Ravindra | Sood | Swamykannu </center>
output:
  html_document:
  code_folding: show
  highlight: monochrome
  theme: flatly
  pdf_document: default
  word_document: default
---

# {.tabset .tabset-fade .tabset-pills}

## Synopsis 
The data was collected and made available by "National Institute of Diabetes and Digestive and Kidney Diseases. Our project focuses on factors which affect the likelihood of person getting the diabetes.
![image](https://3n9hl3wy2ektx6nh3y3tpu15-wpengine.netdna-ssl.com/wp-content/uploads/sites/22/2019/01/diabetestest_647161.jpg)

## Packages Required
```{r List of packages used for the project, ECHO= False}
library(readxl)
library(kableExtra)
library(corrplot)
library(dplyr)
library(ggplot2)
library(GGally)
library(tidyr)
library(DT)
library(gridExtra)
library(ROCR)
library(leaps)
packages <- read_excel("C:/Users/Radhika Sood/Documents/DAM_project/Packages.xlsx")
kable(packages)%>%
kable_styling( bootstrap_options = c("striped", "responsive"))

```

## Data Preparation

### Data Source

The link to original dataset can be found [here](https://www.kaggle.com/kandij/diabetes-dataset)
This dataset contains 768 rows and 9 columns.  
```{r Importing data from URL into the table} 
data <- read.csv("C:/Users/Radhika Sood/Desktop/R datasets/diabetes-dataset/diabetes2.csv", stringsAsFactors = FALSE, header = TRUE)
nrow(data)
ncol(data)
colnames(data)
```
We can understand more about the structure of the dataset by using the str() function.

```{r}
str(data)
```
These 8 variables are indicators taken to consideration of getting diabietes. The 'Outcome' is a response variable stated whether a person has diabetes or not by showing "0" for NO and "1" for Yes.

### Data Cleaning 

Checking for missing value and NULL value in the given dataset is one of the crucial steps in data cleaning 
```{r Check for missing values}
    any(is.na(data))
    any(is.null(data))
    
```
The result is False so we do not have to worry about the missing values and NULL value might affects building our model later on.
Next step, we have to search for dupplicated records in the data set.
```{r Check for duplicate values}
duplicate_value <- data[duplicated(data),]
head(unique(data))

sapply(data, function(x) length(unique(x)))
```
Using duplicate function, we can find the duplicate values in the row. It returns FALSE for each row indicating no duplicates. We verified it by using the Unique() function, the unique function returns the unique rows in the dataset. We get 768 rows. Thus, we have no duplicate values in the data.

The next thing we need to look for is the outliers of each variable.
```{r}
a <- ggplot(data, aes(x= Outcome, y= Pregnancies, fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom") 
b<- ggplot(data, aes(x= Outcome, y= Glucose, fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom")
c<- ggplot(data, aes(x= Outcome, y= BloodPressure , fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom")
d<- ggplot(data, aes(x= Outcome, y= SkinThickness , fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom")
e<- ggplot(data, aes(x= Outcome, y= Insulin, fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom") 
f<- ggplot(data, aes(x= Outcome, y= BMI, fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom") 
g<- ggplot(data, aes(x= Outcome, y= DiabetesPedigreeFunction, fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom")
h<- ggplot(data, aes(x= Outcome, y= Age, fill = factor(Outcome))) + geom_boxplot() + theme(legend.position = "bottom") 
grid.arrange(a,b,c,d,e,f,g,h, ncol=4)
```



We can see that women with non diabetes (outcome =0) has fewer number of pregnancies compared to the diabetes group and is skew to the right. The diabetes women group have higher Glucose concentration. The two group have quite similar blood pressure measurement. There are many outliers in Insulin from both women groups, especially women with diabetes is heavily skewed to the right. Women with diabetes have slightly higher BMI than the other.The pedigree function distribution in both groups have outliers and have positive skew.The average age of women in diabetes group seems older than women in non diabetes group. 

In reality, living organisms can't have zero value for their blood pressure. We will check if there how many rows that contains 0 value in Blood Pressure.
```{r}
sum(data$BloodPressure==0)
```
With fasting glucose levels would not be as low as zero. Therefore zero is an invalid reading. We will check if there how many rows that contains 0 value in Glucose.
```{r}
sum(data$Glucose==0)
```
For normal people, skin fold thickness can’t be less than 10 mm better yet zero. We will check if there how many rows that contains 0 value.
```{r}
sum(data$SkinThickness==0)
```
So a fasting insulin level should never be 0, which it might be in a person with untreated Type 1. It shouldn’t go below 3. We will check if there how many rows that contains 0 value.
```{r}
sum(data$Insulin==0)
```
BMI can't be or close to 0 cause it is not reality related. We will check if there how many rows that contains 0 value.
```{r}
sum(data$BMI==0)
```
With the domain knowledge checking, we found invalid values in some columns. We conclude that the given data set is imcomplete. This leads to our group decision to use imputation to make the data set more relevant and reasonable. 
We should replace the rows contained zero value in blood pressure, glucose, skin thickness, insulin and BMI variables with the median value to keep the our dataset logically.
```{r Check for incorrect values and replace with relevant values}
# People with no diabetes an zero blood pressure values

non_diabetic <- which(data$BloodPressure==0 & data$Outcome==0)
temp_median <- median(data$BloodPressure[!data$BloodPressure==0 & data$Outcome==0])
data$BloodPressure[non_diabetic] <- temp_median
temp_median

# People with diabetes an zero blood pressure values
diabetic <- which(data$BloodPressure==0 & data$Outcome==1)
data$BloodPressure[diabetic] <- median(data$BloodPressure[!data$BloodPressure==0 & data$Outcome==1])

# People with no diabetes an zero Glucose values
non_diabetic <- which(data$Glucose==0 & data$Outcome==0)
data$Glucose[non_diabetic] <- median(data$Glucose[!data$Glucose==0 & data$Outcome==0])

# People with diabetes an zero Glucose values
diabetic <- which(data$Glucose==0 & data$Outcome==1)
data$Glucose[diabetic] <- median(data$Glucose[!data$Glucose==0 & data$Outcome==1])

# People with no diabetes an zero Skin Thickness values
non_diabetic <- which(data$SkinThickness==0 & data$Outcome==0)
data$SkinThickness[non_diabetic] <- median(data$SkinThickness[!data$SkinThickness==0 & data$Outcome==0])

# People with diabetes an zero Skin Thickness values
diabetic <- which(data$SkinThickness==0 & data$Outcome==1)
data$SkinThickness[diabetic] <- median(data$SkinThickness[!data$SkinThickness==0 & data$Outcome==1])

# People with no diabetes an zero Insulin values
non_diabetic <- which(data$Insulin==0 & data$Outcome==0)
data$Insulin[non_diabetic] <- median(data$Insulin[!data$Insulin==0 & data$Outcome==0])

# People with diabetes an zero Insulin values
diabetic <- which(data$Insulin==0 & data$Outcome==1)
data$Insulin[diabetic] <- median(data$Insulin[!data$Insulin==0 & data$Outcome==1])

# People with no diabetes an zero BMI values
non_diabetic <- which(data$BMI==0 & data$Outcome==0)
data$BMI[non_diabetic] <- median(data$BMI[!data$BMI==0 & data$Outcome==0])

# People with diabetes an zero BMI values
diabetic <- which(data$BMI==0 & data$Outcome==1)
data$BMI[diabetic] <- median(data$BMI[!data$BMI==0 & data$Outcome==1])

summary(data)
```

  
## Exploratory Data Analysis
### Check correlation between the variables 
```{r}
pairs(data,pch=20)
corrplot(cor(data[,-10]),type = "lower", method="number")
ggplot(gather(data[,-9]),aes(value)) + geom_histogram() + facet_wrap(key~.,scales="free_x")
```

By looking at the correlation mattrices, we can see that the data dots in Skin Thickness might have correlation with BMI. The coefficient in the correlation plot shows 0.57 between BMI and Skin Thickness which means there is a moderate positive relationship. It also shows correlation coefficient of 0.54 between age and pregnancies and 0.49 between Insulin and Glucose. These coefficient measures the strength and direction of a linear relationship between two variables. However, these coefficents we are getting from the scatterplot is not strong enough to assure that their are a significant relationship among the covariates. So we can do further analysis without dropping any columns.

From the histogram, we can visualize the distribution in each regressor. BloodPressure and BMI plots seem to follow a normal distribution. Pregnancies, Age, Insulin and Diebetes Pedigree Function are skewed to the right.

## Variable Selection and final Model 

### Fitting the model using all variables 
```{r}
# Building model with all variables
model = glm(Outcome ~ .,data,family="binomial")
summary(model)
```

```{r}
# Selecting significant variables based on p-value
model1 = glm(Outcome ~ .-BloodPressure-Age,data,family="binomial")
summary(model1)
```

### Selecting using BIC value
```{r}

subset_result <- regsubsets(Outcome~.,data, nbest=2, nvmax = 14)
summary(subset_result)
plot(subset_result, scale="bic")

# Model based on BIC
model2 = glm(Outcome~.-BloodPressure-SkinThickness-Age,data=data,family="binomial")
summary(model2)
```



### Using backward selection
```{r}
fullmodel=lm(Outcome~., data=data)
model_step_b <- step(fullmodel,direction='backward')

# Variables based on least AIC
model3 = glm(Outcome ~ Pregnancies + Glucose + SkinThickness + Insulin + BMI + 
               DiabetesPedigreeFunction,data,family="binomial")
summary(model3)
```


### Selecting the final model based on least AIC amongst the above three model.
```{r}
final_model = glm(Outcome ~ .-BloodPressure-Age,data,family="binomial")
summary(final_model)
```

As we interpret estimated coefficent of our significant covariates, we come up with the conclusion that when a person have pregnancy, increasing Glucose, Insulin, BMI,DiabetesPedigreeFunction  and Skin Thickness will likely have diabetes.

The model will look like:

Outcome = -9.22 + 0.13* Pregnancies + 0.03* Glucose + 0.04* SkinThickness + 0.005* Insulin + 0.05* BMI + 0.80*DiabetesPedigreeFunction

To evaluate the prediction performance on all the observed data

```{r}
d = data
pred_prob = predict(final_model, data, type = "response")
pred_value = 1*(pred_prob>0.5)
c=cbind(d, pred_prob, pred_value)
```

We can obtain the confusion matrix as follows.

```{r}
actual_value=d$Outcome
confusion_matrix=table(actual_value,pred_value)
confusion_matrix
```

We can further obtain the misclassification error rate as follows.

```{r}
misclassification_error_rate=1-sum(diag(confusion_matrix))/sum(confusion_matrix)
misclassification_error_rate
```

To check if 0.5 is a good p-cut value.
```{r}
# define a cost function with input "obs" being observed response 
# and "pi" being predicted probability, and "pcut" being the threshold.
costfunc = function(obs, pred.p, pcut){
    weight1 = 1   # define the weight for "true=1 but pred=0" (FN)
    weight0 = 1    # define the weight for "true=0 but pred=1" (FP)
    c1 = (obs==1)&(pred.p<pcut)    # count for "true=1 but pred=0"   (FN)
    c0 = (obs==0)&(pred.p>=pcut)   # count for "true=0 but pred=1"   (FP)
    cost = mean(weight1*c1 + weight0*c0)  # misclassification with weight
    return(cost) # you have to return to a value when you write R functions
} # end of the function

# define a sequence from 0.01 to 1 by 0.01
p.seq = seq(0.01, 1, 0.01) 

mean_error = rep(0, length(p.seq))  
for(i in 1:length(p.seq)){ 
    mean_error[i] = costfunc(obs = d$Outcome, pred.p = pred_prob, pcut = p.seq[i])  
} # end of the loop

# draw a plot with X axis being all pcut and Y axis being associated cost
plot(p.seq, mean_error)
```


Let’s confirm when p-cut is 0.5, we get the previous mean error:
```{r}
mean_error[which(p.seq==0.5)]
```

What’s the best p-cut and lowest mean error we can get?
```{r}
optimal.pcut = p.seq[which(mean_error==min(mean_error))]
optimal.pcut
min(mean_error)
```

Thus, let's use 0.35 as new p-cut value and check the misclassification error.
```{r}
pred_prob = predict(final_model, d, type = "response")
pred_value = 1*(pred_prob>0.35)
c=cbind(d, pred_prob, pred_value)
```


```{r}
actual_value=d$Outcome
confusion_matrix=table(actual_value,pred_value)
confusion_matrix
```

```{r}
misclassification_error_rate=1-sum(diag(confusion_matrix))/sum(confusion_matrix)
misclassification_error_rate
```

Therefore,


The final model will look like:

Outcome = -9.22 + 0.13* Pregnancies + 0.03* Glucose + 0.04* SkinThickness + 0.005* Insulin + 0.05* BMI + 0.80*DiabetesPedigreeFunction

and we use p-cut of 0.35 which gives the misclassification error rate of 0.2005.


## Residual Analysis
### Let's do the residual analysis to check if our assumptions are met.
```{r}
final_model = glm(Outcome ~ .-BloodPressure-Age,data,family="binomial")
par(mfrow=c(1,2))
qqnorm(final_model$residuals,main="model1")
qqline(final_model$residuals)

plot(final_model$fitted.values,final_model$residuals,pch=20)
abline(h=0,col="grey")
```

As we can see, it's violating our assumptions of Normality and Equal variance.

Let's do some transformations to meet these assumptions.
```{r}
final_model = glm(Outcome ~ sqrt(Pregnancies+Glucose+SkinThickness+Insulin+BMI+DiabetesPedigreeFunction),data,family="binomial")
```

Will do the residual analysis again.

```{r}
par(mfrow=c(1,2))
# generate QQ plot
qqnorm(final_model$residuals,main="model1")
qqline(final_model$residuals)

plot(final_model$fitted.values,final_model$residuals,pch=20)
abline(h=0,col="grey")
```

As we can see in the above plots, the assumption of equal variance is met. Though the Normality assumption is not completely met, we still proceed further for model validation as we have huge number of observations which would compensate for non linearity.


## Model Validation

## Model Accuracy
